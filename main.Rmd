---
title: "Tensor diagrams"
description: |
  Penrose notation for deep learning.
author:
  - name: Piotr Migdał
    url: https://p.migdal.pl
    affiliation: Quantum Flytrap
    affiliation_url: https://quantumgame.io
  - name: Claudia Zendejas-Morales
    url: http://claudiazm.xyz
    affiliation: Quantum Flytrap
    affiliation_url: https://quantumflytrap.com
date: "`r Sys.Date()`"
bibliography: references.bib
output:
  distill::distill_article:
    toc: true
    toc_float: true
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r echo=FALSE, message=FALSE, results='asis'}
cat(
  "<script src='https://unpkg.com/tensor-diagrams@0.2.3/dist/esbuild/tensor-diagrams.js'></script>",
  "<link rel='stylesheet' href='tensorDiagram.css'>"
)
```

Multidimenstional arrays are the basic building blocks for any deep neural network, and many other models in statistics and machine learning.
However, even for relatively simple operations the notation becomes monstrous:


$$
\sum_{i_1=1}^n \sum_{i_2=1}^n \sum_{i_3=1}^n \sum_{i_4=1}^n
p_{i_1} T_{i_1 i_2} O_{i_2 j_1}  T_{i_2 i_3} O_{i_3 j_2} T_{i_3 i_4} O_{i_4 j_3}
$$

If you are aleary faimiliar with the problem, you may guess that it is probability of observing $(j_1, j_2, j_3)$ sequence in a Hidden Markov Model. If you are not, it takes time and effort to parse, and comprehend the formula.
In fact, I made a typo when writing it for the first time.

This notation has some problems, as:

- We introduce indices just to sum over them.
- We need to manually inspect which indices repeat (and how many times).
- There is no overview over the struture.


In this expository article we introduce tensor diagram notation in the context of deep learning.
That is - we focus on array of real numbers and relevant operations.
We present it as a convenient notation for matrix summation, nothing less more more.
In this notation, the described equation becomes

<div id="example1" class="equation"></div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("p", {x: 0, y: 0}, [], ["i1"])
  .addTensor("T", {x: 1, y: 0}, ["i1"], ["tx"])
  .addTensor("dot", {x: 2, y: 0}, ["tx"], ["xt"], [], ["xo"], { shape:"dot", showLabel: false })
  .addTensor("T", {x: 3, y: 0}, ["xt"], ["ty"])
  .addTensor("dot", {x: 4, y: 0}, ["ty"], ["yt"], [], ["yo"], { shape:"dot", showLabel: false })
  .addTensor("T", {x: 5, y: 0}, ["yt"], ["tz"])
  .addTensor("dot", {x: 6, y: 0}, ["tz"], [], [], ["zo"], { shape:"dot", showLabel: false })
  .addTensor("O", {x: 2, y: 1}, [], [], ["xo"], ["j1"], { labelPos: "left"} )
  .addTensor("O", {x: 4, y: 1}, [], [], ["yo"], ["j2"], { labelPos: "left"} )
  .addTensor("O", {x: 6, y: 1}, [], [], ["zo"], ["j3"], { labelPos: "left"} )
  .addContraction(0, 1, "i1")
  .addContraction(1, 2, "tx")
  .addContraction(2, 3, "xt")
  .addContraction(2, 7, "xo")
  .addContraction(3, 4, "ty")
  .addContraction(4, 5, "yt")
  .addContraction(4, 8, "yo")
  .addContraction(5, 6, "tz")
  .addContraction(6, 9, "zo")
  .setSize(400, 200)
  .draw("#example1");

```

In this article we use **tensor** as in `torch.Tensor` or TensorFlow - i.e. multidimensional arrays of numebrs with  some additional structure. This a very specific case of would a mathematican call a tensor.
In this context **tensor product** is **outer product**.
(A note for mathematical purists and fetishists: here we work in a finite-dimensional Hilbert space over real numbers, in a fixed basis.)
Also, in physics tensors have a need to fullfil certain transformation criteria [II 02: Differential Calculus of Vector Fields from The Feynman Lectures on Physics](https://www.feynmanlectures.caltech.edu/):

> it is not generally true that any three numbers form a vector. It is true only if, when we rotate the coordinate system, the components of the vector transform among themselves in the correct way.




## Background


Tensor diagrams where invented by Penrose[@penrose_applications_1971].
For the first contact with tensor diagrams I suggest glaring at the beautiful diagrams[@bradley_matrices_2019]. If you have some background in quantum mechanics, go for a short introduciton[@biamonte_tensor_2017] or a slightly longer[@bridgeman_hand-waving_2017].

For a complete introduction I suggest book[@coecke_picturing_2017] or lecture notes [@biamonte_lectures_2020].

Tensor diagrams are popular quantum state decomposition for condensed matter physics[@orus_practical_2014,@verstraete_matrix_2008].


## Key parts

### Tensors


A scalar $c$, a vector $v_i$, a matrix $M_{ij}$ and a (third-order) tensor $T_{ijk}$ are represented by:

<div id="example2" class="equation"></div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("a", {x: 0, y: 0}, [], [])
  .addTensor("v", {x: 2, y: 0}, [], ["i"])
  .addTensor("M", {x: 5, y: 0}, ["i"], ["j"])
  .addTensor("T", {x: 8, y: 0}, ["i"], ["k"], [], ["j"])
  .setSize(580, 140)
  .draw("#example2");


```

Each loose end correspond to an index.


### Operations

#### scalar product

$\sum_i v_i u_i$ (or traditionally:  $\vec{v} \cdot \vec{u}$, or in quantum mechanics $\langle u | v \rangle$)

<div id="example3" class="equation"></div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("v", {x: 0, y: 0}, [], ["i"])
  .addTensor("u", {x: 1, y: 0}, ["i"], [])
  .addContraction(0, 1, "i")
  .setSize(100, 120)
  .draw("#example3");

```


#### Vector times matrix

$\sum_i v_i M_{ij}$ (or traditionally:  $\vec{u} M$)

<div id="example4" class="equation"></div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("u", {x: 0, y: 0}, [], ["i"])
  .addTensor("M", {x: 1, y: 0}, ["i"], ["j"])
  .addContraction(0, 1, "i")
  .setSize(160, 120)
  .draw("#example4");

```

#### Matrix times matrix

$\sum_j M_{ij} M_{jk}$ (or traditionally:  $M N$)

<div id="example5" class="equation"></div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("M", {x: 1, y: 0}, ["i"], ["j"])
  .addTensor("N", {x: 2, y: 0}, ["j"], ["k"])
  .addContraction(0, 1, "j")
  .setSize(220, 120)
  .draw("#example5");

```

#### Expected value

$\sum_{ij} v_i M_{ij} v_j$ (or in the context of quantum: $\langle v | M | v \rangle$)

<div id="example6" class="equation"></div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("v", {x: 0, y: 0}, [], ["i"])
  .addTensor("M", {x: 1, y: 0}, ["i"], ["j"])
  .addTensor("v", {x: 2, y: 0}, ["j"], [])
  .addContraction(0, 1, "i")
  .addContraction(1, 2, "j")
  .setSize(160, 120)
  .draw("#example6");

```


- trace
- tensor product
- batch matrix product

### Black dot

Let's introduce a symbol for a tensor with all 1s.

In the case of no dimensions, it is just 1, and it is not that interesting.
For $v = (1, 1, \ldots, 1)$.
$M$ is an identity matrix or [Kronecker delta](https://en.wikipedia.org/wiki/Kronecker_delta).
$T$ is a tensor for which $T_{ijk} = 0$ for all $i = j = k$, otherwise there are 0s.

<div id="example7" class="equation"></div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("dot", {x: 0, y: 0}, [], [], [], [], { shape:"dot", showLabel: false })
  .addTensor("dot", {x: 2, y: 0}, [], ["i"], [], [], { shape:"dot", showLabel: false })
  .addTensor("dot", {x: 5, y: 0}, ["i"], ["j"], [], [], { shape:"dot", showLabel: false })
  .addTensor("dot", {x: 8, y: 0}, ["i"], ["k"], [], ["j"], { shape:"dot", showLabel: false })
  .setSize(580, 140)
  .draw("#example7");

```

#### Matrix is diagonal

<div id="example8" class="equation"></div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("dot", {x: 1, y: 0}, ["i"], ["j"], [], ["k"], { shape:"dot", showLabel: false })
  .addTensor("S", {x: 1, y: 1}, [], [], ["k"], [], { labelPos: "left"} )
  .addContraction(0, 1, "k")
  .setSize(160, 150)
  .draw("#example8");

```

- normalized probabilities (and other momemnts)
- matrix is stochastic
- matrix is double stochastic
- events are independent

### Models and properties

- SVD
- Markov Chain
- Hidden Markov Chain
- Restricted Boltzman machine
- Ising model

### Deep learning

- batch processing
- convolution
- separable convolution
- batch/etc normalization




## You may have seen tensor diagrams

Feynman Diagrams[@kaiser_physics_2005] (for unbounded operators on infitely-dimensional Hilbert spaces) and quantum computing.

TODO: Do LSTM diagrams qualify?

## Hidden Markov Models

Hidden Markov Models[@rabiner_tutorial_1989].

## More 


## Footnote

Written in [R Markdown Distill](https://rstudio.github.io/distill).

Other inspirations:

- [Einsum is All you Need - Einstein Summation in Deep Learning - Tim Rocktäschel](https://rockt.github.io/2018/04/30/einsum)
- [Simple diagrams of convoluted neural networks - Piotr Migdał](https://medium.com/inbrowserai/simple-diagrams-of-convoluted-neural-networks-39c097d2925b)
- [Tensor Diagram Notation - tensornetwork.org](http://tensornetwork.org/diagrams/) for graphic style
- [Thinking in tensors, writing in PyTorch (a hands-on deep learning intro)](https://github.com/stared/thinking-in-tensors-writing-in-pytorch), especially chapter 1 (it is an eternal Work in Progress, though)
- [bra-ket-vue](https://github.com/Quantum-Game/bra-ket-vue) - a visualizer for quantum states and matrices, aware of the tensor structure
- [Tensor diagram notation in D3.js - JSFiddle](https://jsfiddle.net/stared/8huz5gy7/) - initial version of vis
- [HN on Terry Tao on some desirable properties of mathematical notation](https://news.ycombinator.com/item?id=23911903) - including that "Is it just me, or does probability theory in general have fairly terrible notation?"

# Claudia's part



### Multiply two matrices followed by calculating the sum of each column resulting in a vector

<div id="einsumindl1" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}\sum_{ik}$$</div>
  <div class="eq-elem tensor-eq-A">$$A_{ik}$$</div>
  <div class="eq-elem tensor-eq-B">$$B_{kj}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("A", {x: 1, y: 0}, ["i"], ["k"])
  .addTensor("B", "right", ["k"], ["j"])
  .addContraction(0, 1, "k")
  .setSize(220, 120)
  .draw("#einsumindl1");

```

### Dot product of two vectors

<div id="einsumindl2" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}\sum_{i}$$</div>
  <div class="eq-elem tensor-eq-a">$$a_{i}$$</div>
  <div class="eq-elem tensor-eq-b">$$b_{i}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("a", "start", [], ["i"])
  .addTensor("b", "right", ["i"], [])
  .addContraction(0, 1, "i")
  .setSize(100, 120)
  .draw("#einsumindl2");

```

### Applying a transformation to vectors in a higher-order tensor

<div id="einsumindl3" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}\sum_{k}$$</div>
  <div class="eq-elem tensor-eq-T">$$T_{ntk}$$</div>
  <div class="eq-elem tensor-eq-W">$$W_{kq}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("T", {x: 1, y: 0}, ["n"], ["k"], [], ["t"])
  .addTensor("W", "right", ["k"], ["q"])
  .addContraction(0, 1, "k")
  .setSize(220, 130)
  .draw("#einsumindl3");

```

### Order-4 tensor and project vectors in the 3rd dimension

<div id="einsumindl4" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}\sum_{tk}$$</div>
  <div class="eq-elem tensor-eq-T">$$T_{ntkm}$$</div>
  <div class="eq-elem tensor-eq-W">$$W_{kq}$$</div>
</div>

<!-- This diagram has the problem that the label of the first tensor is behind the index "up" or "left" -->

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("T", {x: 1, y: 0}, ["n"], ["k"], ["m"], ["t"], { labelPos: "left" })
  .addTensor("W", "right", ["k"], ["q"])
  .addContraction(0, 1, "k")
  .setSize(220, 140)
  .draw("#einsumindl4");

```

### Matrix transpose


<div id="einsumindl5_1" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}$$</div>
  <div class="eq-elem tensor-eq-B">$$B_{ji}$$</div>
</div>

<div id="einsumindl5_2" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}$$</div>
  <div class="eq-elem tensor-eq-A">$$A_{ij}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("B", {x: 1, y: 0}, ["j"], ["i"])
  .setSize(160, 110)
  .draw("#einsumindl5_1");
  
TensorDiagram.new()
  .addTensor("A", {x: 1, y: 0}, ["i"], ["j"])
  .setSize(160, 110)
  .draw("#einsumindl5_2");

```

### Sum

<div id="einsumindl6" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}\sum_{ij}$$</div>
  <div class="eq-elem tensor-eq-A">$$A_{ij}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("dot", {x: 0, y: 0}, [], ["i"], [], [], { shape: "dot", showLabel: false })
  .addTensor("A", {x: 1, y: 0}, ["i"], ["j"], [], [])
  .addTensor("dot", {x: 2, y: 0}, ["j"], [], [], [], { shape: "dot", showLabel: false })
  .addContraction(0, 1, "i")
  .addContraction(1, 2, "j")
  .setSize(160, 110)
  .draw("#einsumindl6");

```

### Column sum

<div id="einsumindl7" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}\sum_{i}$$</div>
  <div class="eq-elem tensor-eq-A">$$A_{ij}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("A", {x: 1, y: 0}, ["i"], ["j"], [], [])
  .setSize(160, 110)
  .draw("#einsumindl7");

```

### Row sum

<div id="einsumindl8" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}\sum_{j}$$</div>
  <div class="eq-elem tensor-eq-A">$$A_{ij}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("A", {x: 1, y: 0}, ["i"], ["j"], [], [])
  .addTensor("dot", {x: 2, y: 0}, ["j"], [], [], [], { shape: "dot", showLabel: false })
  .addContraction(0, 1, "j")
  .setSize(160, 110)
  .draw("#einsumindl8");

```

### Matrix-vector multiplication

<div id="einsumindl9" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}\sum_{k}$$</div>
  <div class="eq-elem tensor-eq-A">$$A_{ik}$$</div>
  <div class="eq-elem tensor-eq-b">$$b_{k}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("A", {x: 1, y: 0}, ["i"], ["k"])
  .addTensor("b", "right", ["k"], [])
  .addContraction(0, 1, "k")
  .setSize(160, 120)
  .draw("#einsumindl9");

```

### Matrix-matrix multiplication

<div id="einsumindl10" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}\sum_{k}$$</div>
  <div class="eq-elem tensor-eq-A">$$A_{ik}$$</div>
  <div class="eq-elem tensor-eq-B">$$B_{kj}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("A", {x: 1, y: 0}, ["i"], ["k"])
  .addTensor("B", "right", ["k"], ["j"])
  .addContraction(0, 1, "k")
  .setSize(220, 120)
  .draw("#einsumindl10");

```

### Dot product: Vector

<div id="einsumindl11" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}\sum_{i}$$</div>
  <div class="eq-elem tensor-eq-a">$$a_{i}$$</div>
  <div class="eq-elem tensor-eq-b">$$b_{i}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("a", "start", [], ["i"])
  .addTensor("b", "right", ["i"], [])
  .addContraction(0, 1, "i")
  .setSize(100, 120)
  .draw("#einsumindl11");

```

### Dot product: Matrix

<div id="einsumindl12" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}\sum_{ij}$$</div>
  <div class="eq-elem tensor-eq-A">$$A_{ij}$$</div>
  <div class="eq-elem tensor-eq-B">$$B_{ij}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("A", {x: 1, y: 0}, ["i"], ["j"])
  .addTensor("B", "right", ["j"], ["i"])
  .addContraction(0, 1, "i", "down")
  .addContraction(0, 1, "j")
  .setSize(220, 120)
  .draw("#einsumindl12");

```

### Hadamard product

<div id="einsumindl13" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}$$</div>
  <div class="eq-elem tensor-eq-A">$$A_{ij}$$</div>
  <div class="eq-elem tensor-eq-B">$$B_{ij}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("dot", {x: 1, y: 1}, ["i"], ["k","m"], [], [], { shape:"dot", showLabel: false })
  .addTensor("A", {x: 2, y: 0}, ["k"], ["l"])
  .addTensor("B", {x: 2, y: 2}, ["m"], ["n"])
  .addTensor("dot", {x: 3, y: 1}, ["l","n"], ["j"], [], [], { shape:"dot", showLabel: false })
  .addContraction(0, 1, "k", "left")
  .addContraction(0, 2, "m", "left")
  .addContraction(1, 3, "l", "left")
  .addContraction(2, 3, "n", "left")
  .setSize(270, 220)
  .draw("#einsumindl13");

```

### Outer product

<div id="einsumindl14_1" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}$$</div>
  <div class="eq-elem tensor-eq-a">$$a_{i}$$</div>
  <div class="eq-elem tensor-eq-b">$$b_{j}$$</div>
</div>

<div id="einsumindl14_2" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}$$</div>
  <div class="eq-elem tensor-eq-C">$$C_{ij}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("a", {x: 1, y: 0}, ["i"], [])
  .addTensor("b", {x: 2, y: 0}, [], ["j"])
  .setSize(220, 120)
  .draw("#einsumindl14_1");
  
TensorDiagram.new()
  .addTensor("C", {x: 1, y: 0}, ["i"], ["j"])
  .setSize(210, 120)
  .draw("#einsumindl14_2");

```

### Batch matrix multiplication

<div id="einsumindl15" class="equation">
  <div class="eq-diagram"></div>
  <div class="eq-elem">$$\hspace{0.3cm}=\hspace{0.3cm}\sum_{k}$$</div>
  <div class="eq-elem tensor-eq-A">$$A_{ijk}$$</div>
  <div class="eq-elem tensor-eq-B">$$B_{ikl}$$</div>
</div>

```{js, results='asis', echo=FALSE, message=FALSE}

TensorDiagram.new()
  .addTensor("A", {x: 1, y: 0}, ["i"], ["k"], [], ["j"])
  .addTensor("B", "right", ["k"], ["l"], [], ["i"])
  .addContraction(0, 1, "k")
  .setSize(220, 140)
  .draw("#einsumindl15");

```


## TODO

Idea:

- Linear algebra.
- Copy operation.
- Stochastic models.
- Tensors in the sense of TensorFlow and torch.tensor.